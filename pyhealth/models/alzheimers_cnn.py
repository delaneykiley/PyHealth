# -*- coding: utf-8 -*-
"""Copy of Alzheimer MRI : CNN(0.96%)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/111hD9yuzGjWjFPWknzy4FNKAAEu0KISE
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
borhanitrash_alzheimer_mri_disease_classification_dataset_path = kagglehub.dataset_download('borhanitrash/alzheimer-mri-disease-classification-dataset')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# üß† **Alzheimer‚Äôs MRI Disease Classification** üè•  

## üìå **What is Alzheimer‚Äôs Disease?**  
Alzheimer‚Äôs is a **progressive brain disorder** that causes **memory loss, confusion, and difficulty in thinking**. It is the **leading cause of dementia**, affecting millions worldwide. Over time, it worsens, making daily life challenging.  

## üîç **Why is Early Detection Important?**  
‚úÖ **Slows Disease Progression:** Early diagnosis allows better symptom management.  
‚úÖ **Better Planning:** Helps patients and families prepare for the future.  
‚úÖ **Access to Clinical Trials:** Increases chances of trying new treatments.  
‚úÖ **Reduces Healthcare Costs:** Early intervention lowers medical expenses.  

## üí° **How Can AI Help?**  
Using **MRI scans and Deep Learning (CNNs)**, we can **automate Alzheimer‚Äôs detection**, making diagnosis **faster and more accurate**.  

üéØ **Goal:** Build a **CNN model** to classify MRI scans into different stages of Alzheimer‚Äôs disease. üöÄ

# üìÇ **Dataset Overview**  

  
This dataset contains **MRI scans** for classifying different stages of Alzheimer‚Äôs disease. The images are stored in **Parquet format**, where each row represents an image in a byte-encoded format.


## üîç **Dataset Contents**  
‚úÖ **MRI Images:** Brain scans converted to NumPy arrays.  
‚úÖ **Labels:** Four categories indicating the stage of Alzheimer‚Äôs.  
‚úÖ **Train & Test Split:** Predefined train and test datasets.  

## üè∑ **Class Labels**  
Each MRI scan is classified into **one of four categories**:  

| Label | Class Name             | Description |
|-------|------------------------|-------------|
| **0** | Mild Demented          | Early signs of dementia. |
| **1** | Moderate Demented      | Noticeable memory loss and confusion. |
| **2** | Non Demented           | Healthy brain, no signs of Alzheimer‚Äôs. |
| **3** | Very Mild Demented     | Minimal symptoms, slight cognitive decline. |

This structured dataset helps in training a **Deep Learning model (CNN)** to classify MRI scans accurately. üß†‚ú®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Updated file paths
train = "/kaggle/input/alzheimer-mri-disease-classification-dataset/Alzheimer MRI Disease Classification Dataset/Data/train-00000-of-00001-c08a401c53fe5312.parquet"  # Corrected path
test = "/kaggle/input/alzheimer-mri-disease-classification-dataset/Alzheimer MRI Disease Classification Dataset/Data/test-00000-of-00001-44110b9df98c5585.parquet"

df_train = pd.read_parquet(train)
df_test = pd.read_parquet(test)

#  each image is directly stored as compressed JPEG byte data instead of file paths.
df_train.head()
#this image data is stored as dict key:value key=byte,value=

#print(type(image_bytes))
#<class 'dict'>

"""<h2>Data Preprocessing</h2>

The dataset stores MRI scan images in Parquet format, where images are saved as byte strings inside a dictionary. Since deep learning models require numerical image arrays, we need to convert these byte-encoded images into usable grayscale images.

* How It Works?
* Extracts the byte string from the dictionary.
* Converts bytes into a NumPy array (np.frombuffer).
* Decodes it into an image using cv2.imdecode(), ensuring it‚Äôs in grayscale mode.
* If the input is not a valid dictionary, it raises an error.
"""

import cv2
def dict_to_image(image_dict):
    if isinstance(image_dict, dict) and 'bytes' in image_dict:
        byte_string = image_dict['bytes']
        nparr = np.frombuffer(byte_string, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)
        return img
    else:
        raise TypeError(f"Expected dictionary with 'bytes' key, got {type(image_dict)}")

"""We can apply this function to transform all images in the dataset"""

df_train['img_arr'] = df_train['image'].apply(dict_to_image)
df_train.drop("image", axis=1, inplace=True)
df_train.head()

# Create a dictionary to map labels to class names
label_mapping = {
    0: "Mild_Demented",
    1: "Moderate_Demented",
    2: "Non_Demented",
    3: "Very_Mild_Demented"
}


df_train['class_name'] = df_train['label'].map(label_mapping)


df_train.head()

"""<h3>Explore data</h3>"""

fig, ax = plt.subplots(2, 3, figsize=(15, 5))
axs = ax.flatten()
for axes in axs:
    rand = np.random.randint(0, len(df_train))
    axes.imshow(df_train.iloc[rand]['img_arr'], cmap="gray")
    axes.set_title([df_train.iloc[rand]['class_name']])
plt.tight_layout()
plt.show()

import seaborn as sns



plt.figure(figsize=(8, 5))

# Countplot to visualize the distribution of classes
sns.countplot(data=df_train, x='class_name', palette="viridis")

# Add labels and title
plt.xlabel("Alzheimer Category")
plt.ylabel("Number of Images")
plt.title("Distribution of Alzheimer MRI Categories")
plt.xticks(rotation=20)


plt.show()

df_train['class_name'].value_counts()

"""*dataset is imbalanced because non_demented and ver_mild_demented dominate the dataset while moderate_demented is extremly low*

"""

from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(df_train, test_size=0.2, stratify=df_train['class_name'], random_state=42)

train_df.shape,val_df.shape

"""* Normalize images:
When working with image data, pixel values typically range from 0 to 255
(since images are stored as 8-bit integers). Normalizing them by dividing by 255 scales these values between 0 and 1.

"""

train_df['img_arr'] = train_df['img_arr'].apply(lambda x: x / 255.0)
val_df['img_arr'] = val_df['img_arr'].apply(lambda x: x / 255.0)

print("Training Set Class Distribution:\n", train_df['class_name'].value_counts())
print("\nValidation Set Class Distribution:\n", val_df['class_name'].value_counts())

"""<h1>Model Building</h1>

<h1><u>Baseline model with  CNN:</h1>
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix



X_train = np.stack(train_df['img_arr'].values)  # Convert to NumPy array
y_train = train_df['label'].values

X_val = np.stack(val_df['img_arr'].values)
y_val = val_df['label'].values

# Reshape X (assuming grayscale images of size 128x128)
X_train = X_train.reshape(-1, 128, 128, 1)  # (Samples, Height, Width, Channels)
X_val = X_val.reshape(-1, 128, 128, 1)

#  CNN Model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Dropout(0.015),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.Dropout(0.03),
    layers.Flatten(),
    layers.Dense(4, activation='softmax')  # 4 classes
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])



early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Train model with early stopping
history = model.fit(
    X_train, y_train,
    epochs=50,
    validation_data=(X_val, y_val),
    callbacks=[early_stop]
)

model(np.zeros((1, 128, 128, 1)))

model.summary()

# Evaluate Model
y_pred = np.argmax(model.predict(X_val), axis=1)
print(classification_report(y_val, y_pred))

#  Confusion Matrix
plt.figure(figsize=(6,6))
sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Mild', 'Moderate', 'Non-Demented', 'Very Mild'],
            yticklabels=['Mild', 'Moderate', 'Non-Demented', 'Very Mild'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

# Extract loss values
train_loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(train_loss) + 1)

# Plot
plt.figure(figsize=(8, 5))
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')  # 'bo-' = blue circles with a solid line
plt.plot(epochs, val_loss, 'r^-', label='Validation Loss')  # 'r^-' = red triangles with a solid line
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs. Validation Loss')
plt.legend()
plt.grid()
plt.show()



"""<h2>Prediction on Test Data</h2>"""

df_test['img_arr'] = df_test['image'].apply(dict_to_image)
df_test.drop("image", axis=1, inplace=True)

df_test

X_test = np.stack(df_test['img_arr'].values)  # Convert list of arrays to numpy array
y_test = df_test['label'].values  # Extract labels

X_test = X_test.reshape(-1, 128, 128, 1)  # Reshape for CNN

X_test = X_test / 255.0

y_pred_prob = model.predict(X_test)  # Get probability scores
y_pred = np.argmax(y_pred_prob, axis=1)  # Get predicted class

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

print("Classification Report:\n", classification_report(y_test, y_pred))

!pip install --upgrade tf-keras-vis  # Ensure you have the latest version

import tensorflow as tf
from tf_keras_vis.gradcam import Gradcam  # Import directly from gradcam submodule
from tf_keras_vis.utils import normalize
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Function to display the Grad-CAM
def display_gradcam(img, heatmap, alpha=0.4):
    heatmap = cv2.resize(heatmap, (128, 128))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    if img.shape[-1] == 1:
        img = np.repeat(img, 3, axis=-1)

    superimposed_img = heatmap_color * alpha + (img * 255).astype(np.uint8)
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)

    plt.imshow(superimposed_img)
    plt.axis('off')
    plt.show()

# Grad-CAM function
def generate_gradcam(model, image, last_conv_layer_name, predicted_class):
    # Initialize Gradcam
    gradcam = Gradcam(model, model_modifier=None, clone=True)

    # Define the score function (callable)
    def score_fn(output):
        return output[:, predicted_class]  # Get the score for the predicted class

    # Generate the heatmap
    heatmap = gradcam(score_fn, seed_input=image, penultimate_layer="conv2d_6")[0]

    # Normalize the heatmap
    heatmap = normalize(heatmap)

    return heatmap

# Number of images to display (e.g., 3 images)
num_images_to_display = 10

# Loop through the first 3 images in your validation set
for i in range(num_images_to_display):
    # Prepare the image (ensure it has the correct shape)
    img_array = np.expand_dims(X_val[i], axis=0)  # Image from validation set

    # Get the predicted class for this image
    predicted_class = np.argmax(model.predict(img_array))

    # Define the name of the last convolutional layer
    last_conv_layer_name = 'conv2d_6'
    # Generate Grad-CAM heatmap for this image
    heatmap = generate_gradcam(model, img_array, last_conv_layer_name, predicted_class)

    # Display the Grad-CAM for the current image
    print(f"Displaying Grad-CAM for image {i+1}")
    display_gradcam(X_val[i], heatmap)
